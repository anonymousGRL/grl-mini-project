{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "JpohatEX_6_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biphTtDr_3UP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import typing\n",
        "from typing import Optional, Tuple, Union, Callable\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.utils as U\n",
        "import torch_geometric\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch_geometric.datasets as datasets\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.nn import Linear\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "from torch_sparse import SparseTensor, set_diag\n",
        "\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.dense.linear import Linear\n",
        "from torch_geometric.nn.inits import glorot, zeros, reset\n",
        "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
        "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
        "\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets\n"
      ],
      "metadata": {
        "id": "qh-vDppWAbrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='.', name='IMDB-BINARY', transform=torch_geometric.transforms.OneHotDegree(138)).shuffle()\n",
        "\n",
        "# Print information about the dataset\n",
        "print(f'Dataset: {dataset}')"
      ],
      "metadata": {
        "id": "4XbZvaJzB-F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide dataset and create mini-batches\n",
        "train_dataset = dataset[:int(len(dataset)*0.8)]\n",
        "val_dataset   = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
        "test_dataset  = dataset[int(len(dataset)*0.9):]\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'Training set: {len(train_dataset)} graphs')\n",
        "print(f'Validation set: {len(val_dataset)} graphs')\n",
        "print(f'Test set: {len(test_dataset)} graphs')"
      ],
      "metadata": {
        "id": "2YTbNt8eCH9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Graph Isomorphism Layer (AGINConv)"
      ],
      "metadata": {
        "id": "0ubewLEDDPae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AGINConv(MessagePassing):\n",
        "    r\"\"\"The AGIN layer from the `\"Attention, here comes expressivity\"` study, which combines the original \n",
        "    definition of the\n",
        "    :class:`~torch_geometric.conv.GATv2Conv` layer with a learnable update function from\n",
        "    :class:`~torch_geometric.conv.GINConv`.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample, or :obj:`-1` to\n",
        "            derive the size from the first input(s) to the forward method.\n",
        "            A tuple corresponds to the sizes of source and target\n",
        "            dimensionalities.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        nn (torch.nn.Module): A neural network :math:`h_{\\mathbf{\\Theta}}` that \n",
        "            maps node features :obj:`x` of shape :obj:`[-1, in_channels]` to \n",
        "            shape :obj:`[-1, out_channels]`, *e.g.*, defined by \n",
        "            :class:`torch.nn.Sequential`.\n",
        "        heads (int, optional): Number of multi-head-attentions.\n",
        "            (default: :obj:`1`)\n",
        "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
        "            attentions are averaged instead of concatenated.\n",
        "            (default: :obj:`True`)\n",
        "        eps (float, optional): (Initial) :math:`\\epsilon`-value.\n",
        "            (default: :obj:`0.`)\n",
        "        train_eps (bool, optional): If set to :obj:`True`, :math:`\\epsilon`\n",
        "            will be a trainable parameter. (default: :obj:`False`)\n",
        "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
        "            slope. (default: :obj:`0.2`)\n",
        "        dropout (float, optional): Dropout probability of the normalized\n",
        "            attention coefficients which exposes each node to a stochastically\n",
        "            sampled neighborhood during training. (default: :obj:`0`)\n",
        "        edge_dim (int, optional): Edge feature dimensionality (in case\n",
        "            there are any). (default: :obj:`None`)\n",
        "        fill_value (float or Tensor or str, optional): The way to generate\n",
        "            edge features of self-loops (in case :obj:`edge_dim != None`).\n",
        "            If given as :obj:`float` or :class:`torch.Tensor`, edge features of\n",
        "            self-loops will be directly given by :obj:`fill_value`.\n",
        "            If given as :obj:`str`, edge features of self-loops are computed by\n",
        "            aggregating all features of edges that point to the specific node,\n",
        "            according to a reduce operation. (:obj:`\"add\"`, :obj:`\"mean\"`,\n",
        "            :obj:`\"min\"`, :obj:`\"max\"`, :obj:`\"mul\"`). (default: :obj:`\"mean\"`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "\n",
        "    Shapes:\n",
        "        - **input:**\n",
        "          node features :math:`(|\\mathcal{V}|, F_{in})` or\n",
        "          :math:`((|\\mathcal{V_s}|, F_{s}), (|\\mathcal{V_t}|, F_{t}))`\n",
        "          if bipartite,\n",
        "          edge indices :math:`(2, |\\mathcal{E}|)`,\n",
        "          edge features :math:`(|\\mathcal{E}|, D)` *(optional)*\n",
        "        - **output:** node features :math:`(|\\mathcal{V}|, H * F_{out})` or\n",
        "          :math:`((|\\mathcal{V}_t|, H * F_{out})` if bipartite.\n",
        "          If :obj:`return_attention_weights=True`, then\n",
        "          :math:`((|\\mathcal{V}|, H * F_{out}),\n",
        "          ((2, |\\mathcal{E}|), (|\\mathcal{E}|, H)))`\n",
        "          or :math:`((|\\mathcal{V_t}|, H * F_{out}), ((2, |\\mathcal{E}|),\n",
        "          (|\\mathcal{E}|, H)))` if bipartite\n",
        "    \"\"\"\n",
        "    _alpha: OptTensor\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        nn: Callable,\n",
        "        heads: int = 1,\n",
        "        concat: bool = True,\n",
        "        eps: float = 0.0,\n",
        "        train_eps: bool = False,\n",
        "        negative_slope: float = 0.2,\n",
        "        dropout: float = 0.0,\n",
        "        edge_dim: Optional[int] = None,\n",
        "        fill_value: Union[float, Tensor, str] = 'mean',\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.concat = concat\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "        self.add_self_loops = True\n",
        "        self.edge_dim = edge_dim\n",
        "        self.fill_value = fill_value\n",
        "        self.nn = nn\n",
        "        self.initial_eps = eps\n",
        "\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.Tensor([eps]))\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            self.lin = torch.cat([torch.eye(in_channels) for i in range(heads)], 0)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        self.att = Parameter(torch.Tensor(1, heads, in_channels))\n",
        "\n",
        "        if edge_dim is not None:\n",
        "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False,\n",
        "                                   weight_initializer='glorot')\n",
        "        else:\n",
        "            self.lin_edge = None\n",
        "\n",
        "        self._alpha = None\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.lin_edge is not None:\n",
        "            self.lin_edge.reset_parameters()\n",
        "        glorot(self.att)\n",
        "        reset(self.nn)\n",
        "        self.eps.data.fill_(self.initial_eps)\n",
        "\n",
        "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None,\n",
        "                return_attention_weights: bool = None):\n",
        "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, NoneType) -> Tensor  # noqa\n",
        "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, NoneType) -> Tensor  # noqa\n",
        "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
        "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
        "                will additionally return the tuple\n",
        "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
        "                attention weights for each edge. (default: :obj:`None`)\n",
        "        \"\"\"\n",
        "        H, C = self.heads, self.in_channels\n",
        "\n",
        "        x_l: OptTensor = None\n",
        "        x_r: OptTensor = None\n",
        "        if isinstance(x, Tensor):\n",
        "            assert x.dim() == 2\n",
        "            x_l = torch.nn.functional.linear(x, self.lin).view(-1, H, C)\n",
        "            x_r = x_l\n",
        "\n",
        "        assert x_l is not None\n",
        "        assert x_r is not None\n",
        "\n",
        "        if self.add_self_loops:\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                num_nodes = x_l.size(0)\n",
        "                if x_r is not None:\n",
        "                    num_nodes = min(num_nodes, x_r.size(0))\n",
        "                edge_index, edge_attr = remove_self_loops(\n",
        "                    edge_index, edge_attr)\n",
        "                edge_index, edge_attr = add_self_loops(\n",
        "                    edge_index, edge_attr, fill_value=self.fill_value,\n",
        "                    num_nodes=num_nodes)\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                if self.edge_dim is None:\n",
        "                    edge_index = set_diag(edge_index)\n",
        "                else:\n",
        "                    raise NotImplementedError(\n",
        "                        \"The usage of 'edge_attr' and 'add_self_loops' \"\n",
        "                        \"simultaneously is currently not yet supported for \"\n",
        "                        \"'edge_index' in a 'SparseTensor' form\")\n",
        "\n",
        "        # propagate_type: (x: PairTensor, edge_attr: OptTensor)\n",
        "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr,\n",
        "                             size=None, n_nodes = x.size()[0])\n",
        "\n",
        "        alpha = self._alpha\n",
        "        self._alpha = None\n",
        "\n",
        "        out = out.view(-1, C)\n",
        "        out = self.nn(out)\n",
        "        out = out.view(-1, H, self.out_channels)\n",
        "\n",
        "        if self.concat:\n",
        "            out = out.view(-1, H * self.out_channels)\n",
        "        else:\n",
        "            out = out.mean(dim=1)\n",
        "\n",
        "        if isinstance(return_attention_weights, bool):\n",
        "            assert alpha is not None\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                return out, (edge_index, alpha)\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                return out, edge_index.set_value(alpha, layout='coo')\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "    def message(self, x_j: Tensor, x_i: Tensor, edge_attr: OptTensor,\n",
        "                index: Tensor, ptr: OptTensor,\n",
        "                size_i: Optional[int], n_nodes: int) -> Tensor:\n",
        "        x = x_i + x_j\n",
        "        if edge_attr is not None:\n",
        "            if edge_attr.dim() == 1:\n",
        "                edge_attr = edge_attr.view(-1, 1)\n",
        "            assert self.lin_edge is not None\n",
        "            edge_attr = self.lin_edge(edge_attr)\n",
        "            edge_attr = edge_attr.view(-1, self.heads, self.out_channels)\n",
        "            x = x + edge_attr\n",
        "\n",
        "        x = F.leaky_relu(x, self.negative_slope)\n",
        "        alpha = (x * self.att).sum(dim=-1)\n",
        "        alpha = softmax(alpha, index, ptr, size_i)\n",
        "        self._alpha = alpha\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        l = lambda x: 1 if x < alpha.size()[0] - n_nodes else 1 + self.eps\n",
        "        m = torch.tensor([l(i) for i in range(alpha.size()[0])])\n",
        "        m = torch.transpose(m.repeat(self.heads,1),0,1)\n",
        "        alpha = m * alpha\n",
        "        return x_j * alpha.unsqueeze(-1)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, heads={self.heads})')\n"
      ],
      "metadata": {
        "id": "CEtiGJuHDsWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n"
      ],
      "metadata": {
        "id": "M5rHMcu9F3h7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Module\n"
      ],
      "metadata": {
        "id": "OJ_2rSuRGfuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_dim: int,\n",
        "               hid_dim: int,\n",
        "               output_dim: int,\n",
        "               n_layers: int,\n",
        "               dropout_ratio: float = 0.3):\n",
        "    super(GCN, self).__init__()\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input_dim: input feature dimension\n",
        "      hid_dim: hidden feature dimension\n",
        "      output_dim: number of target classes\n",
        "      n_layers: number of layers\n",
        "      dropout_ratio: dropout_ratio\n",
        "    \"\"\"\n",
        "\n",
        "    indim = lambda x: input_dim if x == 0 else hid_dim\n",
        "\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    assert n_layers > 0\n",
        "\n",
        "    self.layers = nn.Sequential(*[GCNConv(indim(i), hid_dim) for i in range(n_layers)])\n",
        "    self.lin1 = Linear(hid_dim, hid_dim)\n",
        "    self.lin2 = Linear(hid_dim, output_dim)\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    # Generate Node Embeddings\n",
        "    for layer in self.layers[:-2]:\n",
        "      x = layer(x, edge_index)\n",
        "      x = F.relu(x)\n",
        "    h = self.layers[-1](x, edge_index)\n",
        "\n",
        "    # Graph-level readout\n",
        "    hG = global_mean_pool(h, batch)\n",
        "\n",
        "    # Classification\n",
        "    h = F.dropout(hG, p = self.dropout_ratio, training = self.training)\n",
        "    h = self.lin1(h)\n",
        "    h = h.relu()\n",
        "    h = F.dropout(h, p = self.dropout_ratio, training = self.training)\n",
        "    h = self.lin2(h)\n",
        "\n",
        "    return hG, F.log_softmax(h, dim = 1)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.layers:\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "    self.lin1.reset_parameters()\n",
        "    self.lin2.reset_parameters()\n",
        "      \n"
      ],
      "metadata": {
        "id": "iMt7ro92GXJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GIN Module\n"
      ],
      "metadata": {
        "id": "smiLWXprMIsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GIN(nn.Module):\n",
        "  def __init__(self,\n",
        "                input_dim: int,\n",
        "                hid_dim: int,\n",
        "                output_dim: int,\n",
        "                n_layers: int,\n",
        "                dropout_ratio: float = 0.3):\n",
        "    super(GIN, self).__init__()\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input_dim: input feature dimension\n",
        "      hid_dim: hidden feature dimension\n",
        "      output_dim: number of target classes\n",
        "      n_layers: number of layers\n",
        "      dropout_ratio: dropout_ratio\n",
        "    \"\"\"\n",
        "\n",
        "    indim = lambda x: input_dim if x == 0 else hid_dim\n",
        "    \n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    assert n_layers > 0\n",
        "\n",
        "    self.layers = nn.Sequential(*[GINConv(nn.Sequential(Linear(indim(i), hid_dim), \n",
        "                                                        nn.BatchNorm1d(hid_dim),\n",
        "                                                        nn.ReLU(),\n",
        "                                                        Linear(hid_dim, hid_dim),\n",
        "                                                        nn.ReLU())) for i in range(n_layers)])\n",
        "    self.lin1 = Linear(hid_dim * n_layers, hid_dim * n_layers)\n",
        "    self.lin2 = Linear(hid_dim * n_layers, output_dim)\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    embeddings = []\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, edge_index)\n",
        "      embeddings.append(x)\n",
        "    \n",
        "    # Graph-level read-out\n",
        "    e = []\n",
        "    for h in embeddings:\n",
        "      h = global_add_pool(h, batch)\n",
        "      e.append(h)\n",
        "    embeddings = e\n",
        "\n",
        "    h = torch.cat(embeddings, dim = 1)\n",
        "    \n",
        "    h = self.lin1(h)\n",
        "    h = h.relu()\n",
        "    h = F.dropout(h, p = self.dropout_ratio, training = self.training)\n",
        "    h = self.lin2(h)\n",
        "\n",
        "    return h, F.log_softmax(h, dim = 1)\n",
        "\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.layers:\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "    self.lin1.reset_parameters()\n",
        "    self.lin2.reset_parameters()\n"
      ],
      "metadata": {
        "id": "vFSqwn_bMIQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GAT Module\n"
      ],
      "metadata": {
        "id": "0OY_pGeYxDsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_dim: int,\n",
        "               hid_dim: int,\n",
        "               output_dim: int,\n",
        "               n_layers: int,\n",
        "               n_heads: int = 1,\n",
        "               dropout_ratio: float = 0.3):\n",
        "    super(GAT, self).__init__()\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input_dim: input feature dimension\n",
        "      hid_dim: hidden feature dimension\n",
        "      output_dim: number of target classes\n",
        "      n_layers: number of layers\n",
        "      n_heads: number of attention heads\n",
        "      dropout_ratio: dropout_ratio\n",
        "    \"\"\"\n",
        "\n",
        "    indim = lambda x: input_dim if x == 0 else hid_dim * n_heads\n",
        "\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    assert n_layers > 0\n",
        "\n",
        "    self.layers = nn.Sequential(*[GATConv(indim(i), hid_dim, heads = n_heads) for i in range(n_layers)])\n",
        "    self.lin1 = Linear(hid_dim * n_heads, hid_dim * n_heads)\n",
        "    self.lin2 = Linear(hid_dim * n_heads, output_dim)\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    # Generate Node Embeddings\n",
        "    for layer in self.layers[:-2]:\n",
        "      x = layer(x, edge_index)\n",
        "      x = F.elu(x)\n",
        "      x = F.dropout(x, self.dropout_ratio, training = self.training)\n",
        "    h = self.layers[-1](x, edge_index)\n",
        "\n",
        "    # Graph-level readout\n",
        "    hG = global_mean_pool(h, batch)\n",
        "\n",
        "    # Classification\n",
        "    h = F.dropout(hG, p = self.dropout_ratio, training = self.training)\n",
        "    h = self.lin1(h)\n",
        "    h = h.relu()\n",
        "    h = F.dropout(h, p = self.dropout_ratio, training = self.training)\n",
        "    h = self.lin2(h)\n",
        "\n",
        "    return hG, F.log_softmax(h, dim = 1)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.layers:\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "    self.lin1.reset_parameters()\n",
        "    self.lin2.reset_parameters()\n",
        "  \n"
      ],
      "metadata": {
        "id": "L75cMX1iw5p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AGIN Module"
      ],
      "metadata": {
        "id": "3fDMapXZ7m4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AGIN(nn.Module):\n",
        "  def __init__(self,\n",
        "                input_dim: int,\n",
        "                hid_dim: int,\n",
        "                output_dim: int,\n",
        "                n_layers: int,\n",
        "                n_heads: int,\n",
        "                dropout_ratio: float = 0.3):\n",
        "    super(AGIN, self).__init__()\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input_dim: input feature dimension\n",
        "      hid_dim: hidden feature dimension\n",
        "      output_dim: number of target classes\n",
        "      n_layers: number of layers\n",
        "      n_heads: number of attention heads\n",
        "      dropout_ratio: dropout_ratio\n",
        "    \"\"\"\n",
        "    \n",
        "    indim = lambda x: input_dim if x == 0 else hid_dim * n_heads\n",
        "    \n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.n_layers = n_layers\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_heads = n_heads\n",
        "    \n",
        "    assert n_layers > 0\n",
        "\n",
        "    self.layers = nn.Sequential(*[AGINConv(indim(i), \n",
        "                                           hid_dim, \n",
        "                                           nn.Sequential(Linear(indim(i), hid_dim), \n",
        "                                                        nn.BatchNorm1d(hid_dim),\n",
        "                                                        nn.ReLU(),\n",
        "                                                        Linear(hid_dim, hid_dim),\n",
        "                                                        nn.ReLU()),\n",
        "                                           heads = n_heads,\n",
        ") for i in range(n_layers)])\n",
        "    self.lin1 = Linear(hid_dim * n_heads * n_layers, hid_dim * n_heads * n_layers)\n",
        "    self.lin2 = Linear(hid_dim * n_heads * n_layers, output_dim)\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    embeddings = []\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, edge_index)\n",
        "      embeddings.append(x)\n",
        "    \n",
        "    # Graph-level read-out\n",
        "    e = []\n",
        "    for h in embeddings:\n",
        "      h = global_add_pool(h, batch)\n",
        "      e.append(h)\n",
        "    embeddings = e\n",
        "\n",
        "    h = torch.cat(embeddings, dim = 1)\n",
        "    \n",
        "    h = self.lin1(h)\n",
        "    h = h.relu()\n",
        "    h = F.dropout(h, p = self.dropout_ratio, training = self.training)\n",
        "    h = self.lin2(h)\n",
        "\n",
        "    return h, F.log_softmax(h, dim = 1)\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.layers:\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "    self.lin1.reset_parameters()\n",
        "    self.lin2.reset_parameters()\n"
      ],
      "metadata": {
        "id": "4kl6a4af7oup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instantiation"
      ],
      "metadata": {
        "id": "Dugcr4Ciqz9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agin = AGIN(dataset.num_features,\n",
        "          8,\n",
        "          dataset.num_classes,\n",
        "          3,\n",
        "          8)\n",
        "gat = GAT(dataset.num_features,\n",
        "          8,\n",
        "          dataset.num_classes,\n",
        "          3,\n",
        "          8)\n",
        "\n",
        "gcn = GCN(dataset.num_features,\n",
        "          64,\n",
        "          dataset.num_classes,\n",
        "          3)\n",
        "\n",
        "gin = GIN(dataset.num_features,\n",
        "          64,\n",
        "          dataset.num_classes,\n",
        "          3)"
      ],
      "metadata": {
        "id": "aD0sg5ecWCjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Pn-_FKgOWWwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader= val_loader, test_loader= test_loader, lr=0.01, weight_decay= 5e-4, epochs= 100, patience=5):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                      lr=lr,\n",
        "                                      weight_decay=weight_decay)\n",
        "    epochs = epochs\n",
        "\n",
        "    model.train()\n",
        "    print(model)\n",
        "    c = 0 # Patience Counter\n",
        "    previous = 0\n",
        "    for epoch in range(epochs+1):\n",
        "        total_loss = 0\n",
        "        acc = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "\n",
        "        # Train on batches\n",
        "        for data in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          _, out = model(data.x, data.edge_index, data.batch)\n",
        "          loss = criterion(out, data.y)\n",
        "          total_loss += loss / len(train_loader)\n",
        "          acc += accuracy(out.argmax(dim=1), data.y) / len(train_loader)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Validation\n",
        "          val_loss, val_acc = test(model, val_loader)\n",
        "        \n",
        "        # Early Stop\n",
        "        if (acc < previous):\n",
        "            c += 1 \n",
        "            if (c > patience):\n",
        "              break\n",
        "        else:\n",
        "            c = 0\n",
        "        previous = acc\n",
        "        \n",
        "        # Print metrics every 10 epochs\n",
        "        if(epoch % 10 == 0):\n",
        "            print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} '\n",
        "              f'| Train Acc: {acc*100:>5.2f}% '\n",
        "              f'| Val Loss: {val_loss:.2f} '\n",
        "              f'| Val Acc: {val_acc*100:.2f}%')\n",
        "          \n",
        "    test_loss, test_acc = test(model, test_loader)\n",
        "    print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')\n",
        "    \n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    for data in loader:\n",
        "        _, out = model(data.x, data.edge_index, data.batch)\n",
        "        loss += criterion(out, data.y) / len(loader)\n",
        "        acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "\n",
        "def accuracy(pred_y, y):\n",
        "    \"\"\"Calculate accuracy.\"\"\"\n",
        "    return ((pred_y == y).sum() / len(y)).item()"
      ],
      "metadata": {
        "id": "oGZHzB7hWaeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agin = train(agin, train_loader, weight_decay=0.01)\n",
        "gat = train(gat, train_loader, weight_decay=0.0005)\n",
        "gcn = train(gcn, train_loader, weight_decay=0.0005)\n",
        "gin = train(gin, train_loader, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "nxyHqH3zieih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation"
      ],
      "metadata": {
        "id": "f-V-lMWAdb5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate(model, dataset= dataset, num_splits=10, batch_size=32, epochs=200, weight_decay = 5e-4):\n",
        "    \"\"\"Use k-fold cross validation to evaluate a model on a dataset.\"\"\"\n",
        "\n",
        "    # Get the number of graphs and number of pairs\n",
        "    size = len(dataset)\n",
        "\n",
        "    # Arrays to store the train and test accuracies for each fold\n",
        "    train_accuracies = np.zeros(num_splits)\n",
        "    test_accuracies = np.zeros(num_splits)\n",
        "    \n",
        "    # Loop over the folds\n",
        "    for fold in range(num_splits): \n",
        "\n",
        "        # Print a header to mark this fold\n",
        "        print(f\"Fold {fold+1}\")\n",
        "        print(\"============================\")\n",
        "        print()\n",
        "\n",
        "        # Calculate the current fold segment indices\n",
        "        index_min = int(size // num_splits * fold)\n",
        "        index_max = int(size // num_splits * (fold + 1))\n",
        "\n",
        "        # Split into train and test datasets\n",
        "\n",
        "        train_dataset = (dataset[:index_min] \n",
        "                         + dataset[index_max:])\n",
        "        test_dataset = dataset[index_min:index_max]\n",
        "\n",
        "        # Turn these into torch_geometric dataloaders\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "        # Reset Parameters\n",
        "        model.reset_parameters()\n",
        "\n",
        "        # Train with these\n",
        "        train(model, train_dataloader, test_dataloader, test_dataloader, weight_decay = weight_decay, epochs = epochs)\n",
        "\n",
        "        # Record the test and train accuracies for the trained model\n",
        "        _, train_accuracies[fold] = test(model, train_dataloader)\n",
        "        _, test_accuracies[fold] = test(model, test_dataloader)\n",
        "    \n",
        "    # Print the Train and test accuracies for each fold\n",
        "    print(f\"{num_splits}-fold validation summary\")\n",
        "    print(\"============================\")\n",
        "    for fold in range(num_splits):\n",
        "        print(f\"Fold {fold+1}. Train: {train_accuracies[fold]:09.5%} \"\n",
        "              f\"Test: {test_accuracies[fold]:09.5%}\")\n"
      ],
      "metadata": {
        "id": "4YlpT3CXdegX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_validate(agin)\n",
        "cross_validate(gat)\n",
        "cross_validate(gcn)\n",
        "cross_validate(gin)"
      ],
      "metadata": {
        "id": "KX0Xr2JTkzGf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}